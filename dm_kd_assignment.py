# -*- coding: utf-8 -*-
"""DM_KD_Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AQ7iWoNlX_DHpqpMWCDsU5pJGy717Mgx
"""

# Import necessary libraries
from tensorflow.keras.datasets import cifar10
from sklearn.model_selection import train_test_split

def load_and_process_cifar10():

    # Load the CIFAR-10 dataset
    (x_train, y_train), (x_test, y_test) = cifar10.load_data()

    # Normalize the images by dividing by 255
    x_train = x_train / 255.0
    x_test = x_test / 255.0

    # Flatten the images from (32, 32, 3) to (3072,)
    x_train_flattened = x_train.reshape(x_train.shape[0], -1)  # Flatten each image to a 1D array
    x_test_flattened = x_test.reshape(x_test.shape[0], -1)  # Flatten each image to a 1D array

    # Split the training data further into training and validation sets
    x_train_flattened, x_val_flattened, y_train, y_val = train_test_split(
        x_train_flattened, y_train, test_size=0.2, random_state=42
    )

    # Return the processed data
    return x_train_flattened, y_train, x_val_flattened, y_val, x_test_flattened, y_test

# Call the function to load, process, and split the dataset
x_train_flattened, y_train, x_val_flattened, y_val, x_test_flattened, y_test = load_and_process_cifar10()

# Print the shapes of the datasets to confirm the structure
print(f"Training data shape (flattened): {x_train_flattened.shape}")  # (40000, 3072)
print(f"Training labels shape: {y_train.shape}")  # (40000, 1)
print(f"Validation data shape (flattened): {x_val_flattened.shape}")  # (10000, 3072)
print(f"Validation labels shape: {y_val.shape}")  # (10000, 1)
print(f"Test data shape (flattened): {x_test_flattened.shape}")  # (10000, 3072)
print(f"Test labels shape: {y_test.shape}")  # (10000, 1)

# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import cifar10
from collections import Counter

# Load the CIFAR-10 dataset
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

# Normalize the images (pixel values between 0 and 1)
x_train = x_train / 255.0
x_test = x_test / 255.0

# Define class names in CIFAR-10 (these are fixed)
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']

# Set up the plot to display random images from each class
plt.figure(figsize=(10, 6))

# Loop through each class and plot random images
for i in range(10):  # 10 classes in CIFAR-10
    class_idx = np.where(y_train == i)[0]  # Get indices for class i
    random_sample_idx = np.random.choice(class_idx)  # Random index for this class
    plt.subplot(2, 5, i+1)  # 2 rows, 5 columns of images
    plt.imshow(x_train[random_sample_idx])  # Display the image
    plt.title(class_names[i])  # Set title as the class name (fixed, no random assignment)
    plt.axis('off')  # Turn off axis labels

# Show the plot with random images from each class
plt.tight_layout()
plt.show()

# Check the class distribution to detect imbalances
class_counts = Counter(y_train.flatten())  # Flatten y_train and count occurrences of each class
print("Class Distribution (Number of samples per class):")
for i in range(10):
    print(f"Class {i} ({class_names[i]}): {class_counts[i]} samples")

from sklearn.utils import shuffle
from sklearn.metrics import accuracy_score, confusion_matrix
import time
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.svm import SVC


# Function to predict and compute accuracy and confusion matrix for SVM with Linear Kernel
def predict_svm_linear(x_train, y_train, x_test, y_test, subset_fraction=0.3, class_names=None):
    # Shuffle the training data
    x_train, y_train = shuffle(x_train, y_train, random_state=42)

    # Use a subset of the training data
    subset_size = int(x_train.shape[0] * subset_fraction)
    x_train_subset = x_train[:subset_size]
    y_train_subset = y_train[:subset_size]

    # Flatten the images for SVM
    x_train_flat = x_train_subset.reshape(x_train_subset.shape[0], -1)
    x_test_flat = x_test.reshape(x_test.shape[0], -1)

    # Start timer
    start_time = time.time()

    # Train the SVM with a linear kernel
    svm_linear = SVC(kernel='linear')
    svm_linear.fit(x_train_flat, y_train_subset.flatten())

    # Predict on the test set
    y_pred = svm_linear.predict(x_test_flat)

    # End timer
    end_time = time.time()
    print(f"SVM Linear Kernel - Time Taken: {end_time - start_time:.2f} seconds")

    # Compute accuracy
    accuracy = accuracy_score(y_test.flatten(), y_pred)
    print(f"SVM Linear Kernel Accuracy: {accuracy * 100:.2f}%")

    # Create DataFrame for predicted vs actual labels
    results_df = pd.DataFrame({
        'True Label': y_test.flatten(),
        'Predicted Label': y_pred
    })
    print("Predictions vs True Labels:")
    print(results_df.head())  # Show the first few rows

    # Compute and plot confusion matrix
    cm = confusion_matrix(y_test.flatten(), y_pred)
    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=True, fmt="d", cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.title("SVM Linear Kernel - Confusion Matrix")
    plt.xlabel("Predicted Labels")
    plt.ylabel("True Labels")
    plt.show()

import time
import pandas as pd
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
# Function to predict and compute accuracy and confusion matrix for SVM with RBF Kernel
def predict_svm_rbf(x_train, y_train, x_test, y_test, subset_fraction=0.3, class_names=None):
    # Use a subset of the training data
    subset_size = int(x_train.shape[0] * subset_fraction)
    x_train_subset = x_train[:subset_size]
    y_train_subset = y_train[:subset_size]

    # Flatten the images for SVM
    x_train_flat = x_train_subset.reshape(x_train_subset.shape[0], -1)
    x_test_flat = x_test.reshape(x_test.shape[0], -1)

    # Start timer
    start_time = time.time()

    # Train the SVM with an RBF kernel
    svm_rbf = SVC(kernel='rbf')
    svm_rbf.fit(x_train_flat, y_train_subset.flatten())

    # Predict on the test set
    y_pred = svm_rbf.predict(x_test_flat)

    # End timer
    end_time = time.time()
    print(f"SVM RBF Kernel - Time Taken: {end_time - start_time:.2f} seconds")

    # Compute accuracy
    accuracy = accuracy_score(y_test.flatten(), y_pred)
    print(f"SVM RBF Kernel Accuracy: {accuracy * 100:.2f}%")

    # Create DataFrame for predicted vs actual labels
    results_df = pd.DataFrame({
        'True Label': y_test.flatten(),
        'Predicted Label': y_pred
    })
    print("Predictions vs True Labels:")
    print(results_df.head())  # Show the first few rows

    # Compute and plot confusion matrix
    cm = confusion_matrix(y_test.flatten(), y_pred)
    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=True, fmt="d", cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.title("SVM RBF Kernel - Confusion Matrix")
    plt.xlabel("Predicted Labels")
    plt.ylabel("True Labels")
    plt.show()

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Function to predict and compute accuracy and confusion matrix for Gaussian Naive Bayes
def predict_gaussian_nb(x_train, y_train, x_test, y_test, subset_fraction=0.3, class_names=None):
    # Shuffle the training data
    x_train, y_train = shuffle(x_train, y_train, random_state=42)
    # Use a subset of the training data
    subset_size = int(x_train.shape[0] * subset_fraction)
    x_train_subset = x_train[:subset_size]
    y_train_subset = y_train[:subset_size]

    # Flatten the images for Naive Bayes
    x_train_flat = x_train_subset.reshape(x_train_subset.shape[0], -1)
    x_test_flat = x_test.reshape(x_test.shape[0], -1)

    # Start timer
    start_time = time.time()

    # Train the Gaussian Naive Bayes classifier
    gnb = GaussianNB()
    gnb.fit(x_train_flat, y_train_subset.flatten())

    # Predict on the test set
    y_pred = gnb.predict(x_test_flat)

    # End timer
    end_time = time.time()
    print(f"Gaussian Naive Bayes - Time Taken: {end_time - start_time:.2f} seconds")

    # Compute accuracy
    accuracy = accuracy_score(y_test.flatten(), y_pred)
    print(f"Gaussian Naive Bayes Accuracy: {accuracy * 100:.2f}%")

    # Create DataFrame for predicted vs actual labels
    results_df = pd.DataFrame({
        'True Label': y_test.flatten(),
        'Predicted Label': y_pred
    })
    print("Predictions vs True Labels:")
    print(results_df.head())  # Show the first few rows

    # Compute and plot confusion matrix
    cm = confusion_matrix(y_test.flatten(), y_pred)
    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=True, fmt="d", cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.title("Gaussian Naive Bayes - Confusion Matrix")
    plt.xlabel("Predicted Labels")
    plt.ylabel("True Labels")
    plt.show()

import time
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import accuracy_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Function to predict and compute accuracy and confusion matrix for ANN with two hidden layers
def predict_ann_with_plot_subset(x_train, y_train, x_test, y_test, subset_fraction=0.3, class_names=None):
    # Shuffle the training data
    x_train, y_train = shuffle(x_train, y_train, random_state=42)
    # Use a subset of the training data
    subset_size = int(x_train.shape[0] * subset_fraction)
    x_train_subset = x_train[:subset_size]
    y_train_subset = y_train[:subset_size]

    # Flatten the images for ANN
    x_train_flat = x_train_subset.reshape(x_train_subset.shape[0], 32, 32, 3)  # No need to flatten for CNN-like models
    x_test_flat = x_test.reshape(x_test.shape[0], 32, 32, 3)  # No need to flatten for CNN-like models

    # One-hot encode the labels
    y_train_one_hot = to_categorical(y_train_subset, 10)
    y_test_one_hot = to_categorical(y_test, 10)

    # Build a simple ANN model with 2 hidden layers
    model = Sequential([
        Flatten(input_shape=(32, 32, 3)),  # Flatten the images
        Dense(512, activation='relu'),     # First hidden layer
        Dense(256, activation='relu'),     # Second hidden layer
        Dense(10, activation='softmax')    # Output layer with 10 classes
    ])

    # Compile the model
    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

    # Start timer
    start_time = time.time()

    # Train the model and store the history
    model.fit(x_train_flat, y_train_one_hot, epochs=100, batch_size=64, validation_data=(x_test_flat, y_test_one_hot))

    # End timer
    end_time = time.time()
    print(f"ANN - Time Taken: {end_time - start_time:.2f} seconds")

    # Predict on the test set
    y_pred = model.predict(x_test_flat)
    y_pred_classes = y_pred.argmax(axis=1)  # Convert probabilities to class labels

    # Compute accuracy
    accuracy = accuracy_score(y_test.flatten(), y_pred_classes)
    print(f"ANN Accuracy: {accuracy * 100:.2f}%")

    # Create DataFrame for predicted vs actual labels
    results_df = pd.DataFrame({
        'True Label': y_test.flatten(),
        'Predicted Label': y_pred_classes
    })
    print("Predictions vs True Labels:")
    print(results_df.head())  # Show the first few rows

    # Compute and plot confusion matrix
    cm = confusion_matrix(y_test.flatten(), y_pred_classes)
    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=True, fmt="d", cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.title("ANN - Confusion Matrix")
    plt.xlabel("Predicted Labels")
    plt.ylabel("True Labels")
    plt.show()

class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']

# Call the function with the necessary parameters
predict_svm_linear(x_train, y_train, x_test, y_test, subset_fraction=0.3, class_names=class_names)

class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
# Call the function with the necessary parameters
predict_svm_rbf(x_train, y_train, x_test, y_test, subset_fraction=0.3, class_names=class_names)

class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
# Call the function with the necessary parameters
predict_gaussian_nb(x_train, y_train, x_test, y_test, subset_fraction=0.3, class_names=class_names)

class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
# Call the function with the necessary parameters
predict_ann_with_plot_subset(x_train, y_train, x_test, y_test, subset_fraction=0.3, class_names=class_names)

from sklearn.model_selection import KFold, StratifiedKFold
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Function for K-Fold and Stratified K-Fold Cross-Validation
def k_fold_svm_linear_subset(x_train, y_train, k=10, stratified=True, subset_fraction=0.3, class_names=None):
    # Shuffle and reduce training data to subset_fraction (30% in this case)
    np.random.seed(42)  # For reproducibility
    indices = np.random.permutation(x_train.shape[0])  # Shuffle indices
    subset_size = int(subset_fraction * x_train.shape[0])  # Calculate subset size
    subset_indices = indices[:subset_size]  # Select subset
    x_train = x_train[subset_indices]
    y_train = y_train[subset_indices]

    # Flatten the training data for SVM
    x_train_flat = x_train.reshape(x_train.shape[0], -1)
    y_train_flat = y_train.flatten()

    # Choose the cross-validation strategy
    if stratified:
        kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)
        print("Using Stratified K-Fold Cross-Validation")
    else:
        kf = KFold(n_splits=k, shuffle=True, random_state=42)
        print("Using Standard K-Fold Cross-Validation")

    accuracies = []
    fold = 1

    # Perform cross-validation
    for train_index, val_index in kf.split(x_train_flat, y_train_flat):
        print(f"\nFold {fold}:")
        x_train_cv, x_val_cv = x_train_flat[train_index], x_train_flat[val_index]
        y_train_cv, y_val_cv = y_train_flat[train_index], y_train_flat[val_index]

        # Train SVM with a linear kernel
        svm_linear = SVC(kernel='linear')
        svm_linear.fit(x_train_cv, y_train_cv)

        # Predict on validation set
        y_pred = svm_linear.predict(x_val_cv)

        # Compute accuracy
        accuracy = accuracy_score(y_val_cv, y_pred)
        accuracies.append(accuracy)
        print(f"Accuracy for Fold {fold}: {accuracy * 100:.2f}%")

        # Display the first 5 actual and predicted labels
        print("First 5 Actual Labels:   ", y_val_cv[:5])
        print("First 5 Predicted Labels:", y_pred[:5])

        # Generate and display confusion matrix for the current fold
        cm = confusion_matrix(y_val_cv, y_pred)
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=class_names if class_names else range(len(cm)),
                    yticklabels=class_names if class_names else range(len(cm)))
        plt.title(f"Confusion Matrix for Fold {fold}")
        plt.xlabel("Predicted Labels")
        plt.ylabel("True Labels")
        plt.show()

        fold += 1

    # Summary of results
    mean_accuracy = np.mean(accuracies)
    std_dev = np.std(accuracies)
    print("\nCross-Validation Results:")
    print(f"Mean Accuracy: {mean_accuracy * 100:.2f}%")
    print(f"Standard Deviation: {std_dev * 100:.2f}%")

    return mean_accuracy, std_dev

# Define class names for CIFAR-10
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']

# Perform Stratified K-Fold Cross-Validation with 30% of training data
mean_acc, std_dev = k_fold_svm_linear_subset(x_train, y_train, k=10, stratified=True, subset_fraction=0.3, class_names=class_names)

from sklearn.model_selection import KFold, StratifiedKFold
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Function for K-Fold and Stratified K-Fold Cross-Validation
def k_fold_svm_rbf_subset(x_train, y_train, k=10, stratified=True, subset_fraction=0.3, class_names=None):
    # Shuffle and reduce training data to subset_fraction (30% in this case)
    np.random.seed(42)  # For reproducibility
    indices = np.random.permutation(x_train.shape[0])  # Shuffle indices
    subset_size = int(subset_fraction * x_train.shape[0])  # Calculate subset size
    subset_indices = indices[:subset_size]  # Select subset
    x_train = x_train[subset_indices]
    y_train = y_train[subset_indices]

    # Flatten the training data for SVM
    x_train_flat = x_train.reshape(x_train.shape[0], -1)
    y_train_flat = y_train.flatten()

    # Choose the cross-validation strategy
    if stratified:
        kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)
        print("Using Stratified K-Fold Cross-Validation")
    else:
        kf = KFold(n_splits=k, shuffle=True, random_state=42)
        print("Using Standard K-Fold Cross-Validation")

    accuracies = []
    fold = 1

    # Perform cross-validation
    for train_index, val_index in kf.split(x_train_flat, y_train_flat):
        print(f"\nFold {fold}:")
        x_train_cv, x_val_cv = x_train_flat[train_index], x_train_flat[val_index]
        y_train_cv, y_val_cv = y_train_flat[train_index], y_train_flat[val_index]

        # Train SVM with an RBF kernel
        svm_rbf = SVC(kernel='rbf')
        svm_rbf.fit(x_train_cv, y_train_cv)

        # Predict on validation set
        y_pred = svm_rbf.predict(x_val_cv)

        # Compute accuracy
        accuracy = accuracy_score(y_val_cv, y_pred)
        accuracies.append(accuracy)
        print(f"Accuracy for Fold {fold}: {accuracy * 100:.2f}%")

        # Display the first 5 actual and predicted labels
        print("First 5 Actual Labels:   ", y_val_cv[:5])
        print("First 5 Predicted Labels:", y_pred[:5])

        # Generate and display confusion matrix for the current fold
        cm = confusion_matrix(y_val_cv, y_pred)
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=class_names if class_names else range(len(cm)),
                    yticklabels=class_names if class_names else range(len(cm)))
        plt.title(f"Confusion Matrix for Fold {fold}")
        plt.xlabel("Predicted Labels")
        plt.ylabel("True Labels")
        plt.show()

        fold += 1

    # Summary of results
    mean_accuracy = np.mean(accuracies)
    std_dev = np.std(accuracies)
    print("\nCross-Validation Results:")
    print(f"Mean Accuracy: {mean_accuracy * 100:.2f}%")
    print(f"Standard Deviation: {std_dev * 100:.2f}%")

    return mean_accuracy, std_dev

# Define class names for CIFAR-10
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
# Example call with RBF kernel
mean_acc, std_dev = k_fold_svm_rbf_subset(x_train, y_train, k=10, stratified=True, subset_fraction=0.3, class_names=class_names)

from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import KFold, StratifiedKFold
from sklearn.metrics import accuracy_score, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Function for K-Fold Cross-Validation with Gaussian Naive Bayes
def k_fold_gnb_subset(x_train, y_train, k=10, stratified=True, subset_fraction=0.3, class_names=None):
    # Shuffle and reduce training data to subset_fraction (30% in this case)
    np.random.seed(42)  # For reproducibility
    indices = np.random.permutation(x_train.shape[0])  # Shuffle indices
    subset_size = int(subset_fraction * x_train.shape[0])  # Calculate subset size
    subset_indices = indices[:subset_size]  # Select subset
    x_train = x_train[subset_indices]
    y_train = y_train[subset_indices]

    # Flatten the training data for GNB
    x_train_flat = x_train.reshape(x_train.shape[0], -1)
    y_train_flat = y_train.flatten()

    # Choose the cross-validation strategy
    if stratified:
        kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)
        print("Using Stratified K-Fold Cross-Validation")
    else:
        kf = KFold(n_splits=k, shuffle=True, random_state=42)
        print("Using Standard K-Fold Cross-Validation")

    accuracies = []
    fold = 1

    # Perform cross-validation
    for train_index, val_index in kf.split(x_train_flat, y_train_flat):
        print(f"\nFold {fold}:")
        x_train_cv, x_val_cv = x_train_flat[train_index], x_train_flat[val_index]
        y_train_cv, y_val_cv = y_train_flat[train_index], y_train_flat[val_index]

        # Train Gaussian Naive Bayes
        gnb = GaussianNB()
        gnb.fit(x_train_cv, y_train_cv)

        # Predict on validation set
        y_pred = gnb.predict(x_val_cv)

        # Compute accuracy
        accuracy = accuracy_score(y_val_cv, y_pred)
        accuracies.append(accuracy)
        print(f"Accuracy for Fold {fold}: {accuracy * 100:.2f}%")

        # Display the first 5 actual and predicted labels
        print("First 5 Actual Labels:   ", y_val_cv[:5])
        print("First 5 Predicted Labels:", y_pred[:5])

        # Generate and display confusion matrix for the current fold
        cm = confusion_matrix(y_val_cv, y_pred)
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=class_names if class_names else range(len(cm)),
                    yticklabels=class_names if class_names else range(len(cm)))
        plt.title(f"Confusion Matrix for Fold {fold}")
        plt.xlabel("Predicted Labels")
        plt.ylabel("True Labels")
        plt.show()

        fold += 1

    # Summary of results
    mean_accuracy = np.mean(accuracies)
    std_dev = np.std(accuracies)
    print("\nCross-Validation Results:")
    print(f"Mean Accuracy: {mean_accuracy * 100:.2f}%")
    print(f"Standard Deviation: {std_dev * 100:.2f}%")

    return mean_accuracy, std_dev

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import StratifiedKFold, KFold
from sklearn.metrics import accuracy_score
import numpy as np

# Function for K-Fold Cross-Validation with ANN
def k_fold_ann_subset(x_train, y_train, k=10, stratified=True, subset_fraction=0.3, class_names=None):
    # Shuffle and reduce training data to subset_fraction (30% in this case)
    np.random.seed(42)  # For reproducibility
    indices = np.random.permutation(x_train.shape[0])  # Shuffle indices
    subset_size = int(subset_fraction * x_train.shape[0])  # Calculate subset size
    subset_indices = indices[:subset_size]  # Select subset
    x_train = x_train[subset_indices]
    y_train = y_train[subset_indices]

    # Normalize and one-hot encode labels
    y_train_onehot = to_categorical(y_train, num_classes=10)

    # Choose the cross-validation strategy
    if stratified:
        kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)
        print("Using Stratified K-Fold Cross-Validation")
    else:
        kf = KFold(n_splits=k, shuffle=True, random_state=42)
        print("Using Standard K-Fold Cross-Validation")

    accuracies = []
    fold = 1

    # Perform cross-validation
    for train_index, val_index in kf.split(x_train, y_train):
        print(f"\nFold {fold}:")
        x_train_cv, x_val_cv = x_train[train_index], x_train[val_index]
        y_train_cv, y_val_cv = y_train_onehot[train_index], y_train[val_index]

        # Build ANN model
        model = Sequential([
            Flatten(input_shape=(x_train_cv.shape[1], x_train_cv.shape[2], x_train_cv.shape[3])),
            Dense(512, activation='relu'),
            Dense(256, activation='relu'),
            Dense(10, activation='softmax')
        ])

        model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

        # Train the ANN model
        model.fit(x_train_cv, y_train_cv, epochs=10, batch_size=32, verbose=0)

        # Predict on validation set
        y_pred = np.argmax(model.predict(x_val_cv), axis=1)

        # Compute accuracy
        accuracy = accuracy_score(y_val_cv, y_pred)
        accuracies.append(accuracy)
        print(f"Accuracy for Fold {fold}: {accuracy * 100:.2f}%")

        # Display the first 5 actual and predicted labels
        print("First 5 Actual Labels:   ", y_val_cv[:5])
        print("First 5 Predicted Labels:", y_pred[:5])

        fold += 1

    # Summary of results
    mean_accuracy = np.mean(accuracies)
    std_dev = np.std(accuracies)
    print("\nCross-Validation Results:")
    print(f"Mean Accuracy: {mean_accuracy * 100:.2f}%")
    print(f"Standard Deviation: {std_dev * 100:.2f}%")

    return mean_accuracy, std_dev

mean_acc, std_dev = k_fold_gnb_subset(x_train, y_train, k=10, stratified=True, subset_fraction=0.3, class_names=class_names)

mean_acc, std_dev = k_fold_ann_subset(x_train, y_train, k=10, stratified=True, subset_fraction=0.3, class_names=class_names)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import StratifiedKFold, KFold
from sklearn.metrics import accuracy_score
import numpy as np
import matplotlib.pyplot as plt

# Function for K-Fold Cross-Validation with ANN
def k_fold_ann_with_graph(x_train, y_train, k=10, stratified=True, subset_fraction=0.3, class_names=None):
    # Shuffle and reduce training data to subset_fraction (30% in this case)
    np.random.seed(42)  # For reproducibility
    indices = np.random.permutation(x_train.shape[0])  # Shuffle indices
    subset_size = int(subset_fraction * x_train.shape[0])  # Calculate subset size
    subset_indices = indices[:subset_size]  # Select subset
    x_train = x_train[subset_indices]
    y_train = y_train[subset_indices]

    # Normalize and one-hot encode labels
    y_train_onehot = to_categorical(y_train, num_classes=10)

    # Choose the cross-validation strategy
    if stratified:
        kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)
        print("Using Stratified K-Fold Cross-Validation")
    else:
        kf = KFold(n_splits=k, shuffle=True, random_state=42)
        print("Using Standard K-Fold Cross-Validation")

    accuracies = []
    fold = 1

    # Perform cross-validation
    for train_index, val_index in kf.split(x_train, y_train):
        print(f"\nFold {fold}:")
        x_train_cv, x_val_cv = x_train[train_index], x_train[val_index]
        y_train_cv, y_val_cv = y_train_onehot[train_index], y_train[val_index]

        # Build ANN model
        model = Sequential([
            Flatten(input_shape=(x_train_cv.shape[1], x_train_cv.shape[2], x_train_cv.shape[3])),
            Dense(512, activation='relu'),
            Dense(256, activation='relu'),
            Dense(10, activation='softmax')
        ])

        model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

        # Train the ANN model
        history = model.fit(x_train_cv, y_train_cv, epochs=100, batch_size=32, verbose=0, validation_data=(x_val_cv, to_categorical(y_val_cv, num_classes=10)))

        # Plot the training loss and validation loss
        plt.figure(figsize=(12, 6))
        plt.plot(history.history['loss'], label='Training Loss')
        plt.plot(history.history['val_loss'], label='Validation Loss')
        plt.title(f"Fold {fold} - Loss vs Epochs")
        plt.xlabel("Epochs")
        plt.ylabel("Loss")
        plt.legend()
        plt.show()

        # Predict on validation set
        y_pred = np.argmax(model.predict(x_val_cv), axis=1)

        # Compute accuracy
        accuracy = accuracy_score(y_val_cv, y_pred)
        accuracies.append(accuracy)
        print(f"Accuracy for Fold {fold}: {accuracy * 100:.2f}%")

        # Display the first 5 actual and predicted labels
        print("First 5 Actual Labels:   ", y_val_cv[:5])
        print("First 5 Predicted Labels:", y_pred[:5])

        fold += 1

    # Summary of results
    mean_accuracy = np.mean(accuracies)
    std_dev = np.std(accuracies)
    print("\nCross-Validation Results:")
    print(f"Mean Accuracy: {mean_accuracy * 100:.2f}%")
    print(f"Standard Deviation: {std_dev * 100:.2f}%")

    return mean_accuracy, std_dev

mean_acc, std_dev = k_fold_ann_with_graph(x_train, y_train, k=10, stratified=True, subset_fraction=0.3, class_names=class_names)